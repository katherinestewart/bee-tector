{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b67f7a0",
   "metadata": {},
   "source": [
    "# New baseline with 12?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48492f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 09:07:48.795740: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-09-03 09:07:50.399005: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/thahyra/.pyenv/versions/lewagon/lib/python3.12/site-packages/tensorflow/python/keras/engine/training_arrays_v1.py:37: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 2.3.2)\n",
      "  from scipy.sparse import issparse  # pylint: disable=g-import-not-at-top\n",
      "2025-09-03 09:07:55.651161: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Had to add this stuff to get notebook to find files, because it is not\n",
    "# located in the root folder\n",
    "ROOT = Path.cwd().parent\n",
    "sys.path.insert(0, str(ROOT))\n",
    "\n",
    "from keras import Sequential, Input, layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras.utils import load_img, img_to_array\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "\n",
    "from bee_tector.config import (\n",
    "    FULL_DATA_DIR,\n",
    "    IMAGE_SIZE,\n",
    "    CURATED_DATA_DIR,\n",
    "    MODELS_DIR\n",
    ")\n",
    "from bee_tector.plots import plot_history\n",
    "from bee_tector.data import (\n",
    "    load_datasets,\n",
    "    undersample_dataset,\n",
    "    load_selected_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77e40809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3619 files belonging to 12 classes.\n",
      "Found 781 files belonging to 12 classes.\n",
      "Found 781 files belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds, test_ds = load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "891d7f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_rgb(image, label):\n",
    "    if image.shape[-1] != 3:\n",
    "        image = tf.image.grayscale_to_rgb(image)\n",
    "    return image, label\n",
    "\n",
    "train_ds = train_ds.map(ensure_rgb)\n",
    "val_ds = val_ds.map(ensure_rgb)\n",
    "test_ds = test_ds.map(ensure_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6a2d4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "safetensors==0.5.2\n",
      "tensorboard==2.20.0\n",
      "tensorboard-data-server==0.7.2\n",
      "tensorflow==2.20.0\n",
      "tensorflow-datasets==4.9.7\n",
      "tensorflow-metadata==1.16.1\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad461853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def better_model(shape=(224, 224, 3), num_classes=12):\n",
    "\n",
    "    data_augmentation = Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.15),\n",
    "    layers.RandomZoom(0.2),\n",
    "    layers.RandomContrast(0.1),\n",
    "    layers.RandomTranslation(0.1, 0.1)\n",
    "        ], name=\"data_augmentation\")\n",
    "    # print(shape)\n",
    "    # loading a pretrainded model tf.keras.applications.EfficientNetB0\n",
    "    base_model = EfficientNetB0(\n",
    "    include_top=False, # Exclude the original ImageNet classification head\n",
    "    weights='imagenet', # Use pretrained weights from ImageNet\n",
    "    input_shape=(224, 224, 3)  # Input shape of your images\n",
    "            )\n",
    "    # this freezes the model to Don't update the weights of the base model during training\n",
    "    # this way only the newly added layers  will be trained\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model = Sequential(name=\"BeeClassifier\")\n",
    "\n",
    "    model.add(Input(shape=shape))\n",
    "    model.add(data_augmentation)\n",
    "    model.add(layers.Rescaling(1./255))  # RESCALE!\n",
    "    model.add(base_model)\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "\n",
    "    reg = regularizers.l2(1e-5)\n",
    "    model.add(layers.Dense(128, activation='relu', kernel_regularizer=reg))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-4),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ef08606",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape mismatch in layer #1 (named stem_conv)for weight stem_conv/kernel. Weight expects shape (3, 3, 1, 32). Received saved weight with shape (3, 3, 3, 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbetter_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 12\u001b[0m, in \u001b[0;36mbetter_model\u001b[0;34m(shape, num_classes)\u001b[0m\n\u001b[1;32m      3\u001b[0m data_augmentation \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[1;32m      4\u001b[0m layers\u001b[38;5;241m.\u001b[39mRandomFlip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhorizontal\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      5\u001b[0m layers\u001b[38;5;241m.\u001b[39mRandomRotation(\u001b[38;5;241m0.15\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m layers\u001b[38;5;241m.\u001b[39mRandomTranslation(\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m      9\u001b[0m     ], name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_augmentation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# print(shape)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# loading a pretrainded model tf.keras.applications.EfficientNetB0\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m base_model \u001b[38;5;241m=\u001b[39m \u001b[43mEfficientNetB0\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m\u001b[49m\u001b[43minclude_top\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Exclude the original ImageNet classification head\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m\u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimagenet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Use pretrained weights from ImageNet\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Input shape of your images\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# this freezes the model to Don't update the weights of the base model during training\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# this way only the newly added layers  will be trained\u001b[39;00m\n\u001b[1;32m     19\u001b[0m base_model\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.12/site-packages/keras/src/applications/efficientnet.py:571\u001b[0m, in \u001b[0;36mEfficientNetB0\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, name)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\n\u001b[1;32m    556\u001b[0m     [\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.applications.efficientnet.EfficientNetB0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    569\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mefficientnetb0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    570\u001b[0m ):\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mEfficientNet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_top\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_top\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpooling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpooling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclassifier_activation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclassifier_activation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mb0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.12/site-packages/keras/src/applications/efficientnet.py:434\u001b[0m, in \u001b[0;36mEfficientNet\u001b[0;34m(width_coefficient, depth_coefficient, default_size, dropout_rate, drop_connect_rate, depth_divisor, activation, blocks_args, name, include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, weights_name)\u001b[0m\n\u001b[1;32m    427\u001b[0m     file_name \u001b[38;5;241m=\u001b[39m name \u001b[38;5;241m+\u001b[39m file_suffix\n\u001b[1;32m    428\u001b[0m     weights_path \u001b[38;5;241m=\u001b[39m file_utils\u001b[38;5;241m.\u001b[39mget_file(\n\u001b[1;32m    429\u001b[0m         file_name,\n\u001b[1;32m    430\u001b[0m         BASE_WEIGHTS_PATH \u001b[38;5;241m+\u001b[39m file_name,\n\u001b[1;32m    431\u001b[0m         cache_subdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    432\u001b[0m         file_hash\u001b[38;5;241m=\u001b[39mfile_hash,\n\u001b[1;32m    433\u001b[0m     )\n\u001b[0;32m--> 434\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_weights(weights)\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.12/site-packages/keras/src/legacy/saving/legacy_h5_format.py:452\u001b[0m, in \u001b[0;36m_set_weights\u001b[0;34m(instance, symbolic_weights, weight_values, name, skip_mismatch)\u001b[0m\n\u001b[1;32m    442\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    443\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping loading weights for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    444\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdue to mismatch in shape for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m                 stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    450\u001b[0m             )\n\u001b[1;32m    451\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 452\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    453\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape mismatch in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    454\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor weight \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msymbolic_weights[i]\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    455\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeight expects shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    456\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived saved weight \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    457\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreceived_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    458\u001b[0m         )\n\u001b[1;32m    459\u001b[0m     symbolic_weights[i]\u001b[38;5;241m.\u001b[39massign(weight_value)\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(instance, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinalize_state\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m symbolic_weights:\n",
      "\u001b[0;31mValueError\u001b[0m: Shape mismatch in layer #1 (named stem_conv)for weight stem_conv/kernel. Weight expects shape (3, 3, 1, 32). Received saved weight with shape (3, 3, 3, 32)"
     ]
    }
   ],
   "source": [
    "model = better_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e8fea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 716ms/step - accuracy: 0.0989 - loss: 3.2191 - val_accuracy: 0.0960 - val_loss: 24.9473\n",
      "Epoch 2/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 704ms/step - accuracy: 0.1213 - loss: 2.8301 - val_accuracy: 0.1536 - val_loss: 8.8309\n",
      "Epoch 3/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 707ms/step - accuracy: 0.1456 - loss: 2.7529 - val_accuracy: 0.1191 - val_loss: 2.5923\n",
      "Epoch 4/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 711ms/step - accuracy: 0.1523 - loss: 2.5204 - val_accuracy: 0.1601 - val_loss: 2.4319\n",
      "Epoch 5/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 708ms/step - accuracy: 0.1697 - loss: 2.4208 - val_accuracy: 0.1805 - val_loss: 2.4100\n",
      "Epoch 6/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 716ms/step - accuracy: 0.1865 - loss: 2.3687 - val_accuracy: 0.1933 - val_loss: 2.4238\n",
      "Epoch 7/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 731ms/step - accuracy: 0.2213 - loss: 2.2858 - val_accuracy: 0.2177 - val_loss: 2.4009\n",
      "Epoch 8/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 707ms/step - accuracy: 0.2222 - loss: 2.2326 - val_accuracy: 0.2023 - val_loss: 2.4541\n",
      "Epoch 9/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 715ms/step - accuracy: 0.2481 - loss: 2.1813 - val_accuracy: 0.1921 - val_loss: 2.4402\n",
      "Epoch 10/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 714ms/step - accuracy: 0.2636 - loss: 2.1138 - val_accuracy: 0.2215 - val_loss: 2.3538\n",
      "Epoch 11/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 713ms/step - accuracy: 0.2868 - loss: 2.0261 - val_accuracy: 0.2087 - val_loss: 2.4264\n",
      "Epoch 12/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 715ms/step - accuracy: 0.3114 - loss: 1.9396 - val_accuracy: 0.2202 - val_loss: 2.4185\n",
      "Epoch 13/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 715ms/step - accuracy: 0.3128 - loss: 1.9006 - val_accuracy: 0.2394 - val_loss: 2.3929\n",
      "Epoch 14/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 713ms/step - accuracy: 0.3382 - loss: 1.8463 - val_accuracy: 0.2215 - val_loss: 2.3797\n",
      "Epoch 15/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 732ms/step - accuracy: 0.3653 - loss: 1.7668 - val_accuracy: 0.2305 - val_loss: 2.4160\n",
      "Epoch 16/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 725ms/step - accuracy: 0.3772 - loss: 1.7390 - val_accuracy: 0.2356 - val_loss: 2.4631\n",
      "Epoch 17/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 722ms/step - accuracy: 0.4034 - loss: 1.6459 - val_accuracy: 0.2318 - val_loss: 2.4427\n",
      "Epoch 18/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 720ms/step - accuracy: 0.4120 - loss: 1.6173 - val_accuracy: 0.2625 - val_loss: 2.4473\n",
      "Epoch 19/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 718ms/step - accuracy: 0.4264 - loss: 1.5642 - val_accuracy: 0.2292 - val_loss: 2.4317\n",
      "Epoch 20/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 711ms/step - accuracy: 0.4291 - loss: 1.5568 - val_accuracy: 0.2177 - val_loss: 2.4359\n"
     ]
    }
   ],
   "source": [
    "baseline_model = better_model_model()\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = baseline_model.fit(\n",
    "    train_ds,\n",
    "    epochs=1000,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[es],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf25187e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1s/step - accuracy: 0.1357 - loss: 2.4840\n",
      "Validation loss: 2.4840, Validation accuracy: 0.1357\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 973ms/step - accuracy: 0.1396 - loss: 2.4839\n",
      "Test loss: 2.4839, Test accuracy: 0.1396\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = baseline_model.evaluate(val_ds)\n",
    "print(f\"Validation loss: {val_loss:.4f}, Validation accuracy: {val_acc:.4f}\")\n",
    "\n",
    "test_loss, test_acc = baseline_model.evaluate(test_ds)\n",
    "print(f\"Test loss: {test_loss:.4f}, Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c178b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9e06d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_model.save(f\"{MODELS_DIR}/baseline_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa141be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO edit shape=IMAGE_SIZE + (3,) for future models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18723d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/thahyra/code/katherinestewart/Bee-tector/bee-tector/raw_data/bombus12_full/test/Red-tailed_Bumble_bee/535031756.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m      2\u001b[0m     FULL_DATA_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRed-tailed_Bumble_bee\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m535031756.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m )\n\u001b[0;32m----> 5\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mIMAGE_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m img_array \u001b[38;5;241m=\u001b[39m img_to_array(img)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# We now have an array (224, 224, 3)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Batch size is 1 for 1 image, our model accepts (batch_size, 224, 224, 3)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Bee_tector/lib/python3.10/site-packages/keras/src/utils/image_utils.py:235\u001b[0m, in \u001b[0;36mload_img\u001b[0;34m(path, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, pathlib\u001b[38;5;241m.\u001b[39mPath):\n\u001b[1;32m    234\u001b[0m         path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(path\u001b[38;5;241m.\u001b[39mresolve())\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    236\u001b[0m         img \u001b[38;5;241m=\u001b[39m pil_image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(f\u001b[38;5;241m.\u001b[39mread()))\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/thahyra/code/katherinestewart/Bee-tector/bee-tector/raw_data/bombus12_full/test/Red-tailed_Bumble_bee/535031756.jpg'"
     ]
    }
   ],
   "source": [
    "img_path = os.path.join(\n",
    "    FULL_DATA_DIR, \"test\", \"Red-tailed_Bumble_bee\", \"535031756.jpg\"\n",
    ")\n",
    "\n",
    "img = load_img(img_path, target_size=IMAGE_SIZE)\n",
    "\n",
    "img_array = img_to_array(img)\n",
    "\n",
    "# We now have an array (224, 224, 3)\n",
    "# Batch size is 1 for 1 image, our model accepts (batch_size, 224, 224, 3)\n",
    "img_array = np.expand_dims(img_array, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84169be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = load_model(MODELS_DIR / \"baseline_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6c09a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = baseline_model.predict(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291f376a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0618692 , 0.12679347, 0.09051337, 0.11306442, 0.12979054,\n",
       "        0.06030054, 0.07622439, 0.09393378, 0.04390454, 0.03508817,\n",
       "        0.06704979, 0.10146785]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2c577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.argmax(pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f5a06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b72605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3619 files belonging to 12 classes.\n",
      "Found 781 files belonging to 12 classes.\n",
      "Found 781 files belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "train_cn, val_cn, test_cn = load_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb2058f",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554bf584",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_cn.class_names\n",
    "id_to_class = {i: name for i, name in enumerate(class_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67893d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Common_Eastern_Bumble_Bee\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted class:\", id_to_class[prediction[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfc92a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
