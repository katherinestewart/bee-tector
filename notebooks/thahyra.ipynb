{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b67f7a0",
   "metadata": {},
   "source": [
    "# New baseline with 12?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48492f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Had to add this stuff to get notebook to find files, because it is not\n",
    "# located in the root folder\n",
    "ROOT = Path.cwd().parent\n",
    "sys.path.insert(0, str(ROOT))\n",
    "\n",
    "from keras import Sequential, Input, layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras.utils import load_img, img_to_array\n",
    "from keras.models import load_model\n",
    "from keras.applications import EfficientNetB0\n",
    "\n",
    "from bee_tector.config import (\n",
    "    FULL_DATA_DIR,\n",
    "    IMAGE_SIZE,\n",
    "    CURATED_DATA_DIR,\n",
    "    MODELS_DIR\n",
    ")\n",
    "from bee_tector.plots import plot_history\n",
    "from bee_tector.data import (\n",
    "    load_datasets,\n",
    "    undersample_dataset,\n",
    "    load_selected_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77e40809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3619 files belonging to 12 classes.\n",
      "Found 781 files belonging to 12 classes.\n",
      "Found 781 files belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds, test_ds = load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad461853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(shape=(224, 224, 3), num_classes=12):\n",
    "\n",
    "    data_augmentation = Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.15),\n",
    "    layers.RandomZoom(0.2),\n",
    "    layers.RandomContrast(0.1),\n",
    "    layers.RandomTranslation(0.1, 0.1)\n",
    "], name=\"data_augmentation\")\n",
    "\n",
    "    # loading a pretrainded model tf.keras.applications.EfficientNetB0\n",
    "    base_model = EfficientNetB0(\n",
    "    include_top=False, # Exclude the original ImageNet classification head\n",
    "    weights='imagenet', # Use pretrained weights from ImageNet\n",
    "    input_shape=shape,  # Input shape of your images\n",
    ")\n",
    "\n",
    "    model.add(Input(shape=shape))\n",
    "    model.add(layers.Rescaling(1./255))  # RESCALE!\n",
    "\n",
    "    model.add(layers.Conv2D(32, (4, 4), padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    # I don't really know what this does but you can do this instead of\n",
    "    # flattening I think\n",
    "    # model.add(layers.GlobalAveragePooling2D())\n",
    "\n",
    "    reg = regularizers.l2(1e-5)\n",
    "    model.add(layers.Dense(128, activation='relu', kernel_regularizer=reg))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(32, activation='relu', kernel_regularizer=reg))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Dense(12, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=Adam(learning_rate=1e-4),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ef08606",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = initialize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90e8fea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 716ms/step - accuracy: 0.0989 - loss: 3.2191 - val_accuracy: 0.0960 - val_loss: 24.9473\n",
      "Epoch 2/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 704ms/step - accuracy: 0.1213 - loss: 2.8301 - val_accuracy: 0.1536 - val_loss: 8.8309\n",
      "Epoch 3/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 707ms/step - accuracy: 0.1456 - loss: 2.7529 - val_accuracy: 0.1191 - val_loss: 2.5923\n",
      "Epoch 4/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 711ms/step - accuracy: 0.1523 - loss: 2.5204 - val_accuracy: 0.1601 - val_loss: 2.4319\n",
      "Epoch 5/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 708ms/step - accuracy: 0.1697 - loss: 2.4208 - val_accuracy: 0.1805 - val_loss: 2.4100\n",
      "Epoch 6/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 716ms/step - accuracy: 0.1865 - loss: 2.3687 - val_accuracy: 0.1933 - val_loss: 2.4238\n",
      "Epoch 7/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 731ms/step - accuracy: 0.2213 - loss: 2.2858 - val_accuracy: 0.2177 - val_loss: 2.4009\n",
      "Epoch 8/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 707ms/step - accuracy: 0.2222 - loss: 2.2326 - val_accuracy: 0.2023 - val_loss: 2.4541\n",
      "Epoch 9/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 715ms/step - accuracy: 0.2481 - loss: 2.1813 - val_accuracy: 0.1921 - val_loss: 2.4402\n",
      "Epoch 10/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 714ms/step - accuracy: 0.2636 - loss: 2.1138 - val_accuracy: 0.2215 - val_loss: 2.3538\n",
      "Epoch 11/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 713ms/step - accuracy: 0.2868 - loss: 2.0261 - val_accuracy: 0.2087 - val_loss: 2.4264\n",
      "Epoch 12/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 715ms/step - accuracy: 0.3114 - loss: 1.9396 - val_accuracy: 0.2202 - val_loss: 2.4185\n",
      "Epoch 13/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 715ms/step - accuracy: 0.3128 - loss: 1.9006 - val_accuracy: 0.2394 - val_loss: 2.3929\n",
      "Epoch 14/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 713ms/step - accuracy: 0.3382 - loss: 1.8463 - val_accuracy: 0.2215 - val_loss: 2.3797\n",
      "Epoch 15/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 732ms/step - accuracy: 0.3653 - loss: 1.7668 - val_accuracy: 0.2305 - val_loss: 2.4160\n",
      "Epoch 16/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 725ms/step - accuracy: 0.3772 - loss: 1.7390 - val_accuracy: 0.2356 - val_loss: 2.4631\n",
      "Epoch 17/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 722ms/step - accuracy: 0.4034 - loss: 1.6459 - val_accuracy: 0.2318 - val_loss: 2.4427\n",
      "Epoch 18/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 720ms/step - accuracy: 0.4120 - loss: 1.6173 - val_accuracy: 0.2625 - val_loss: 2.4473\n",
      "Epoch 19/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 718ms/step - accuracy: 0.4264 - loss: 1.5642 - val_accuracy: 0.2292 - val_loss: 2.4317\n",
      "Epoch 20/1000\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 711ms/step - accuracy: 0.4291 - loss: 1.5568 - val_accuracy: 0.2177 - val_loss: 2.4359\n"
     ]
    }
   ],
   "source": [
    "baseline_model = initialize_model()\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = baseline_model.fit(\n",
    "    train_ds,\n",
    "    epochs=1000,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[es],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf25187e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1s/step - accuracy: 0.1357 - loss: 2.4840\n",
      "Validation loss: 2.4840, Validation accuracy: 0.1357\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 973ms/step - accuracy: 0.1396 - loss: 2.4839\n",
      "Test loss: 2.4839, Test accuracy: 0.1396\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = baseline_model.evaluate(val_ds)\n",
    "print(f\"Validation loss: {val_loss:.4f}, Validation accuracy: {val_acc:.4f}\")\n",
    "\n",
    "test_loss, test_acc = baseline_model.evaluate(test_ds)\n",
    "print(f\"Test loss: {test_loss:.4f}, Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c178b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9e06d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_model.save(f\"{MODELS_DIR}/baseline_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa141be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO edit shape=IMAGE_SIZE + (3,) for future models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f18723d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/thahyra/code/katherinestewart/Bee-tector/bee-tector/raw_data/bombus12_full/test/Red-tailed_Bumble_bee/535031756.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m      2\u001b[0m     FULL_DATA_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRed-tailed_Bumble_bee\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m535031756.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m )\n\u001b[0;32m----> 5\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mIMAGE_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m img_array \u001b[38;5;241m=\u001b[39m img_to_array(img)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# We now have an array (224, 224, 3)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Batch size is 1 for 1 image, our model accepts (batch_size, 224, 224, 3)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Bee_tector/lib/python3.10/site-packages/keras/src/utils/image_utils.py:235\u001b[0m, in \u001b[0;36mload_img\u001b[0;34m(path, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, pathlib\u001b[38;5;241m.\u001b[39mPath):\n\u001b[1;32m    234\u001b[0m         path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(path\u001b[38;5;241m.\u001b[39mresolve())\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    236\u001b[0m         img \u001b[38;5;241m=\u001b[39m pil_image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(f\u001b[38;5;241m.\u001b[39mread()))\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/thahyra/code/katherinestewart/Bee-tector/bee-tector/raw_data/bombus12_full/test/Red-tailed_Bumble_bee/535031756.jpg'"
     ]
    }
   ],
   "source": [
    "img_path = os.path.join(\n",
    "    FULL_DATA_DIR, \"test\", \"Red-tailed_Bumble_bee\", \"535031756.jpg\"\n",
    ")\n",
    "\n",
    "img = load_img(img_path, target_size=IMAGE_SIZE)\n",
    "\n",
    "img_array = img_to_array(img)\n",
    "\n",
    "# We now have an array (224, 224, 3)\n",
    "# Batch size is 1 for 1 image, our model accepts (batch_size, 224, 224, 3)\n",
    "img_array = np.expand_dims(img_array, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84169be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = load_model(MODELS_DIR / \"baseline_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c6c09a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = baseline_model.predict(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "291f376a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0618692 , 0.12679347, 0.09051337, 0.11306442, 0.12979054,\n",
       "        0.06030054, 0.07622439, 0.09393378, 0.04390454, 0.03508817,\n",
       "        0.06704979, 0.10146785]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb2c577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.argmax(pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2f5a06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42b72605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3619 files belonging to 12 classes.\n",
      "Found 781 files belonging to 12 classes.\n",
      "Found 781 files belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "train_cn, val_cn, test_cn = load_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb2058f",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "554bf584",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_cn.class_names\n",
    "id_to_class = {i: name for i, name in enumerate(class_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67893d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Common_Eastern_Bumble_Bee\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted class:\", id_to_class[prediction[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfc92a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bee_tector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
